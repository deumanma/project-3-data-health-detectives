{"cells":[{"cell_type":"markdown","source":["# Setting up a Transfer Learning Architecture for Image Classification"],"metadata":{"id":"tSYmb4pnQmBs"}},{"cell_type":"markdown","source":["Transfer learning is a powerful approach in deep learning, enabling models to leverage knowledge from large datasets and apply it to specific tasks with relatively smaller datasets. This method is especially beneficial in scenarios where data collection and labeling are expensive or time-consuming.\n","\n","Transfer learning architecture can handle  classification of multiple items in an image.Transfer learning involves taking a pre-trained model (usually trained on a large dataset like ImageNet, which contains millions of images across thousands of categories) and repurposing it for a different but related problem.\n","\n","Options for pre-trained models include: ResNet, VGG, MobileNet, Inception and others.\n","\n","For classifying multiple items within a single image, we will frame the problem as a multi-label classification problem. This means the model can predict the presence of multiiple items in an image, not just a single dominant object. In this case, we will seek to identify the presence of four states of brain status: no tumor, glioma tumor, meningioma tumor, and pituitary tumor.\n","\n","The last layer of the pre-trained model will be replaced by a new fully connected layer with four outputs.\n","\n","The activation function of the output layer will be sigmoid, which treats each output as a probability and independently.  This allows for the possibility of multiple items (up to four) can be identified in a single image.\n","\n","The model will be trained to output a vector of probabilities for each item. All vectors will be contained in the final layer as a new fully connected layer with four outputs.\n","\n","The model will be trained to maximize the probability of the correct item.\n"],"metadata":{"id":"1mttEXigEY5i"}},{"cell_type":"markdown","source":["# Section 1  Prepare environment"],"metadata":{"id":"IAyOVJMiQ0m4"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"klHNQQlWaZEv","executionInfo":{"status":"ok","timestamp":1711933775277,"user_tz":240,"elapsed":10606,"user":{"displayName":"Aaron Wood","userId":"17542547050237066908"}}},"outputs":[],"source":["import pandas as pd  # For data manipulation and analysis\n","import numpy as np  # For numerical operations\n","from PIL import Image  # For handling images\n","\n","import zipfile  # For handling ZIP files\n","import os  # For handling directories\n","\n","# Import necessary libraries from Keras.\n","from tensorflow.keras.applications import VGG16  # For loading the VGG16 model pre-trained on ImageNet dataset\n","from tensorflow.keras import layers, models  # For creating custom layers and models\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator  # For image data augmentation and preprocessing\n"]},{"cell_type":"markdown","source":["# Section 2 Load and preprocess image dataset\n","\n"],"metadata":{"id":"-ruUKSEbQ-76"}},{"cell_type":"markdown","source":["Make .zip image dataset accessible for preprocessing."],"metadata":{"id":"bZBUgVyEWplL"}},{"cell_type":"code","source":["# Create an upload-the-dataset-button in your notebook.\n","from google.colab import files\n","uploaded = files.upload()"],"metadata":{"id":"ivW1w0LjWkAF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Unzip contents of zip file into Colab Notebook.\n","\n","# Define the path to the ZIP file. 'archive.zip' is the name of the ZIP file\n","# that you uploaded into your Colab Notebook.\n","zip_path = 'archive.zip'\n","\n","# Extract the contents of the ZIP file into current working director\n","# of Colab Notebook.\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall('.')\n","\n","# Clean up by removing the ZIP file.\n","os.remove(zip_path)\n","\n","# Check if the extraction was successful\n","!tree"],"metadata":{"id":"64UBKvG8W_xE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Access images"],"metadata":{"id":"xHtLxjAbaeBx"}},{"cell_type":"code","source":["# ************ O P T I O N A L *****************\n","# # Create an instance of ImageDataGenerator\n","# datagen = ImageDataGenerator(rescale=1./255)\n","\n","# # Specify the directory where your images are located\n","# image_directory = 'archive'\n","\n","# # Load the images from the directory\n","# train_generator = datagen.flow_from_directory(\n","#     image_directory,\n","#     target_size=(150, 150),\n","#     batch_size=32,\n","#     class_mode='binary'\n","# )\n","\n","# # Print the class indices\n","# print(train_generator.class_indices)\n","\n","# # Print the class names\n","# print(train_generator.class_indices.keys())\n","\n","# # Print the number of classes\n","# print(len(train_generator.class_indices))\n","\n","# # Print the shape of the image\n","# print(train_generator.image_shape)\n","\n","# # Print the shape of the labels\n","# print(train_generator.labels.shape)\n"],"metadata":{"id":"a4f8FsriaeNx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Actual loading of image files for training and validation are handled by 'ImageDataGenerator' class. This class loads images from directories, applyies data preprocessing and augmentation, and serves them to the model in batches."],"metadata":{"id":"rLNabVdMRx3B"}},{"cell_type":"code","source":["# Load the training and validation data generators.\n","\n","# Data augmentation and preprocessing parameters can be adjusted\n","# to improve model generalization. In this setup, train_generator and\n","# validation_generator are responsible for loading images from their\n","# respective directories, applying specified preprocessing and augmentation\n","# transformations, and providing the images in batches to the model during\n","# the training and validation phases\n","\n","# Initialize the ImageDataGenerator for the training data with several data augmentation parameters.\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,  # Rescales the image by dividing pixel values by 255 (normalization to 0-1 range).\n","    rotation_range=40,  # Randomly rotates images within a range of 40 degrees.\n","    width_shift_range=0.2,  # Randomly shifts images horizontally by up to 20% of the image width.\n","    height_shift_range=0.2,  # Randomly shifts images vertically by up to 20% of the image height.\n","    shear_range=0.2,  # Applies shear transformation with an intensity of 20%.\n","    zoom_range=0.2,  # Randomly zooms inside pictures by up to 20%.\n","    horizontal_flip=True,  # Randomly flips images horizontally.\n","    fill_mode='nearest'  # Uses the nearest pixel values to fill gaps that may occur during rotation or shifting.\n",")\n","\n","# Initialize the ImageDataGenerator for the validation data.\n","# Typically, we only rescale the validation data without applying any data augmentation.\n","validation_datagen = ImageDataGenerator(rescale=1./255  # Rescales the image by dividing pixel values by 255 (normalization to 0-1 range).\n",")\n","\n","train_generator = train_datagen.flow_from_directory(\n","    'archive/train',\n","    target_size=(224, 224),  # Match the input size expected by the model\n","    batch_size=32,\n","    class_mode='categorical'  # 'categorical' for multi-class classification\n",")\n","\n","validation_generator = validation_datagen.flow_from_directory(\n","    './data/validation',\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical'\n",")\n","\n","# Now, you can use these generators in model.fit()\n","# model.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n"],"metadata":{"id":"o-hLshVORnoS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Section 3 Configure Pretrained Model"],"metadata":{"id":"iR1mqPxNT7qQ"}},{"cell_type":"markdown","source":["# Section 4 Add Classification Layers"],"metadata":{"id":"gHUucs-bUCkj"}},{"cell_type":"markdown","source":["# Section 5 Compile Model\n","\n","---\n","\n"],"metadata":{"id":"QbZ5mkBuUDKO"}},{"cell_type":"markdown","source":["# Section 6 Train Model"],"metadata":{"id":"ay6GCHLuUD8n"}},{"cell_type":"markdown","source":["# Section 7 Evaluate Model"],"metadata":{"id":"pwygTwYkUEWM"}},{"cell_type":"markdown","source":["# Section 8 Fine-tuning"],"metadata":{"id":"BaLRrn4AUEtp"}},{"cell_type":"markdown","source":[],"metadata":{"id":"EgQoJQQSQipS"}},{"cell_type":"markdown","source":["Prepare environment"],"metadata":{"id":"QgaHpRIH5BKP"}},{"cell_type":"markdown","source":[],"metadata":{"id":"0xZkJuS8QiW2"}},{"cell_type":"markdown","source":["Load base model"],"metadata":{"id":"emxWebqY6H_o"}},{"cell_type":"code","source":["# Load the base model, VGG16, pre-trained on the ImageNet dataset.\n","# 'weights='imagenet'' loads the weights trained on ImageNet.\n","# 'include_top=False' excludes the top (fully connected) layers of the model, making it suitable for feature extraction.\n","# 'input_shape=(224, 224, 3)' sets the expected input size to 224x224 pixels with 3 color channels (RGB).\n","base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"],"metadata":{"id":"d7s8V39fFzhA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Freeze base model"],"metadata":{"id":"OtibeA4KF3a4"}},{"cell_type":"code","source":["# Freeze the base model to prevent its weights from being updated during training.\n","# This is done because we want to utilize the learned features without altering them.\n","base_model.trainable = False"],"metadata":{"id":"rmLplemSF3ua"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create a new model ON TOP of existing frozen pre-trainedVGG16 model"],"metadata":{"id":"4ezoU3O5GFNg"}},{"cell_type":"code","source":["# Create a new model on top of the base model.\n","model = models.Sequential([\n","    base_model,  # The pre-trained VGG16 model as the feature extractor.\n","    layers.GlobalAveragePooling2D(),  # Applies global average pooling to the output of the base model.\n","    layers.Dense(1024, activation='relu'),  # A dense layer with 1024 neurons and ReLU activation function for learning non-linearities.\n","    layers.Dropout(0.2),  # Dropout layer for regularization, dropping out 20% of the input units randomly to reduce overfitting.\n","    layers.Dense(4, activation='softmax')  # Output layer with 4 neurons (one for each class) and softmax activation for multi-class classification.\n","])"],"metadata":{"id":"GLs62OeVGFbv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compile model"],"metadata":{"id":"bEkrrI9tGpft"}},{"cell_type":"code","source":["# Compile the model with the Adam optimizer, categorical crossentropy as the loss function\n","# (suitable for multi-class classification), and track the 'accuracy' metric during training.\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"],"metadata":{"id":"y5yPxNwEGpse"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Leverage Image Data Generator Libraryfor training and **validation**"],"metadata":{"id":"aOa3r59sGqBR"}},{"cell_type":"code","source":["# Need below in more plain speak.\n","# Instantiate ImageDataGenerator for training data with data augmentation parameters.\n","# 'rescale=1./255' scales image pixel values to a range of 0 to 1.\n","# 'data_augmentation_strategy' variable is assumed to define specific data augmentation parameters (e.g., rotation, zoom),\n","# which is not explicitly defined in the snippet and should be replaced with actual parameters or removed.\n","train_datagen = ImageDataGenerator(rescale=1./255, data_augmentation_strategy) # Here, replace or remove 'data_augmentation_strategy'.\n","\n","# Instantiate the ImageDataGenerator for validation data without data augmentation, only rescaling.\n","validation_datagen = ImageDataGenerator(rescale=1./255)"],"metadata":{"id":"gbv3JspBGqPD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"b9dddL6lGqm2"}},{"cell_type":"code","source":["# Assuming 'train_generator' and 'validation_generator' are defined elsewhere in the code to load images from directories\n","# using the flow_from_directory method of the respective ImageDataGenerator instances.\n","# This involves specifying the path to the data, target image size, batch size, and class mode.\n"],"metadata":{"id":"Fv3lYQuDGqzx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Data generators for training and validation\n","train_datagen = ImageDataGenerator(rescale=1./255, data_augmentation_strategy)\n","validation_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# Fit the model\n","# model.fit(train_generator, epochs=epochs, validation_data=validation_generator)"],"metadata":{"id":"rAEVLwBj6MzS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a new model on top of the base model.\n","model = models.Sequential([\n","    base_model,  # The pre-trained VGG16 model as the feature extractor.\n","    layers.GlobalAveragePooling2D(),  # Applies global average pooling to the output of the base model.\n","    layers.Dense(1024, activation='relu'),  # A dense layer with 1024 neurons and ReLU activation function for learning non-linearities.\n","    layers.Dropout(0.2),  # Dropout layer for regularization, dropping out 20% of the input units randomly to reduce overfitting.\n","    layers.Dense(4, activation='softmax')  # Output layer with 4 neurons (one for each class) and softmax activation for multi-class classification.\n","])\n","\n","# Compile the model with the Adam optimizer, categorical crossentropy as the loss function (suitable for multi-class classification),\n","# and track the 'accuracy' metric during training.\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Instantiate the ImageDataGenerator for training data with data augmentation parameters.\n","# 'rescale=1./255' scales the image pixel values to a range of 0 to 1.\n","# The 'data_augmentation_strategy' variable is assumed to define specific data augmentation parameters (e.g., rotation, zoom),\n","# which is not explicitly defined in the snippet and should be replaced with actual parameters or removed.\n","train_datagen = ImageDataGenerator(rescale=1./255)  # Here, replace or remove 'data_augmentation_strategy'.\n","\n","# Instantiate the ImageDataGenerator for validation data without data augmentation, only rescaling.\n","validation_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# Assuming 'train_generator' and 'validation_generator' are defined elsewhere in the code to load images from directories\n","# using the flow_from_directory method of the respective ImageDataGenerator instances.\n","# This involves specifying the path to the data, target image size, batch size, and class mode.\n","\n","# Train the model using the fit method, specifying the training and validation data generators, number of epochs, and other parameters.\n","# 'epochs=epochs' should be replaced with a specific number of epochs, for example, 'epochs=10'.\n","# This line is commented out and should be uncommented and completed with actual variable names and values.\n","# model.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n"],"metadata":{"id":"jNonImo-FK_a"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":378,"status":"ok","timestamp":1701465153237,"user":{"displayName":"Sean Morey","userId":"09649124933964159109"},"user_tz":360},"id":"tArhoxTgb48c","outputId":"b72a7267-7ea8-4baa-eca3-5acc344718ab"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>H1_103a_3.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>H1_38b_2.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>H2_8f_14.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>H1_99a_2.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>H2_49c_2.jpg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            name\n","0  H1_103a_3.jpg\n","1   H1_38b_2.jpg\n","2   H2_8f_14.jpg\n","3   H1_99a_2.jpg\n","4   H2_49c_2.jpg"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Print the first few image filenames\n","path = \"https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_1/datasets/csvs/fungi_files.csv\"\n","\n","filenames_df = pd.read_csv(path)\n","filenames_df.head()"]},{"cell_type":"code","source":[],"metadata":{"id":"seeR0UQq4_Dy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gb-QXZKT4_Mx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hdjc-nw64_T6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MyHJXAjN4_at"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RUKd3xqU4_gq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OjtdYlYL4_kN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"V-rehjhS4_nV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":161,"status":"ok","timestamp":1701465153563,"user":{"displayName":"Sean Morey","userId":"09649124933964159109"},"user_tz":360},"id":"720WE2NBcH_h","outputId":"c812d25b-8566-4c20-bd08-bd7a13854c8f"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>class</th>\n","      <th>sample</th>\n","      <th>image</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>H1_103a_3.jpg</td>\n","      <td>H1</td>\n","      <td>103a</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>H1_38b_2.jpg</td>\n","      <td>H1</td>\n","      <td>38b</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>H2_8f_14.jpg</td>\n","      <td>H2</td>\n","      <td>8f</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>H1_99a_2.jpg</td>\n","      <td>H1</td>\n","      <td>99a</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>H2_49c_2.jpg</td>\n","      <td>H2</td>\n","      <td>49c</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            name class sample image\n","0  H1_103a_3.jpg    H1   103a     3\n","1   H1_38b_2.jpg    H1    38b     2\n","2   H2_8f_14.jpg    H2     8f    14\n","3   H1_99a_2.jpg    H1    99a     2\n","4   H2_49c_2.jpg    H2    49c     2"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# First, remove the .jpg file extension,\n","# then split into three new columns: 'class', 'sample', and 'image'\n","filenames_df[['class', 'sample', 'image']] = filenames_df['name']\\\n","                                                            .str.replace('.jpg', '', regex=False)\\\n","                                                            .str.split('_', expand=True)\n","filenames_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wRCnvdKrcWzt"},"outputs":[],"source":["from google.colab import drive\n","import pickle\n","drive.mount('/content/drive')\n","# For our purposes, we'll select the class column as 'y'\n","y = filenames_df['class']\n","\n","# And we'll export this as another pkl file\n","with open('/content/drive/My Drive/fungi_y.pkl', 'wb') as file:\n","    pickle.dump(y, file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gs8aMSEwyRgL"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1wivgIps57GC4LXh3BgJHlKzp8H_V-H4J","timestamp":1711930337290},{"file_id":"1VZ_4Qh1T6TO-IeOy9l6GlcFWOI-XYj-5","timestamp":1711911757002}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}